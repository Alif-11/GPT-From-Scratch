{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9f6cd67",
   "metadata": {},
   "source": [
    "### Get the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3b30b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download training data text\n",
    "import numpy as np\n",
    "from typing import List, Any\n",
    "with open(\"./training-data.txt\", 'r') as training_data_file:\n",
    "  training_data = training_data_file.read()\n",
    "\n",
    "print(f\"\"\"Training data from the Tiny Shakespeare dataset):\\n\\n\n",
    "{training_data[:4000]}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23524094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data length: 1115393\n",
      "65\n",
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
     ]
    }
   ],
   "source": [
    "print(f\"training data length: {len(training_data)}\")\n",
    "\n",
    "## Get all unique characters from the training_data\n",
    "# Please note - the reasoning for this is that Andrej Karpathy wants to make a \n",
    "# next character predictor. Hence, we need all the characters, not the words.\n",
    "\n",
    "# Don't actually need list here, as sorted returns a new sorted list given an iterable.\n",
    "# We add the list(...) function anyway for readability.\n",
    "unique_characters = sorted(list(set(training_data))) \n",
    "print(len(unique_characters))\n",
    "print(''.join(unique_characters))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46fd8583",
   "metadata": {},
   "source": [
    "### Create our Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af6f18e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, ' ': 1, '!': 2, '$': 3, '&': 4, \"'\": 5, ',': 6, '-': 7, '.': 8, '3': 9, ':': 10, ';': 11, '?': 12, 'A': 13, 'B': 14, 'C': 15, 'D': 16, 'E': 17, 'F': 18, 'G': 19, 'H': 20, 'I': 21, 'J': 22, 'K': 23, 'L': 24, 'M': 25, 'N': 26, 'O': 27, 'P': 28, 'Q': 29, 'R': 30, 'S': 31, 'T': 32, 'U': 33, 'V': 34, 'W': 35, 'X': 36, 'Y': 37, 'Z': 38, 'a': 39, 'b': 40, 'c': 41, 'd': 42, 'e': 43, 'f': 44, 'g': 45, 'h': 46, 'i': 47, 'j': 48, 'k': 49, 'l': 50, 'm': 51, 'n': 52, 'o': 53, 'p': 54, 'q': 55, 'r': 56, 's': 57, 't': 58, 'u': 59, 'v': 60, 'w': 61, 'x': 62, 'y': 63, 'z': 64}\n",
      "{0: '\\n', 1: ' ', 2: '!', 3: '$', 4: '&', 5: \"'\", 6: ',', 7: '-', 8: '.', 9: '3', 10: ':', 11: ';', 12: '?', 13: 'A', 14: 'B', 15: 'C', 16: 'D', 17: 'E', 18: 'F', 19: 'G', 20: 'H', 21: 'I', 22: 'J', 23: 'K', 24: 'L', 25: 'M', 26: 'N', 27: 'O', 28: 'P', 29: 'Q', 30: 'R', 31: 'S', 32: 'T', 33: 'U', 34: 'V', 35: 'W', 36: 'X', 37: 'Y', 38: 'Z', 39: 'a', 40: 'b', 41: 'c', 42: 'd', 43: 'e', 44: 'f', 45: 'g', 46: 'h', 47: 'i', 48: 'j', 49: 'k', 50: 'l', 51: 'm', 52: 'n', 53: 'o', 54: 'p', 55: 'q', 56: 'r', 57: 's', 58: 't', 59: 'u', 60: 'v', 61: 'w', 62: 'x', 63: 'y', 64: 'z'}\n",
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
     ]
    }
   ],
   "source": [
    "# Similar to the Sebastian Raschka video:\n",
    "# Create a mapping between tokens and token ids, and vice versa\n",
    "# - although, again, in Andrej Karpathy's case, our tokens are characters, not words\n",
    "\n",
    "# Simple tokenizer\n",
    "char_to_id = {char:integer_id for char, integer_id in zip(unique_characters, range(len(unique_characters)))}\n",
    "id_to_char = {integer_id:char for char, integer_id in char_to_id.items()}\n",
    "print(char_to_id)\n",
    "print(id_to_char)\n",
    "\n",
    "def encode_char_to_id(string : str) -> List[int]:\n",
    "  return [char_to_id[char] for char in string]\n",
    "\n",
    "def decode_id_to_char(list_of_chars : List[int]) -> str:\n",
    "  return ''.join([id_to_char[integer_id] for integer_id in list_of_chars])\n",
    "#print(unique_characters)\n",
    "\n",
    "# encode_char_to_id and decode_id_to_char are inverse functions of each other\n",
    "print(decode_id_to_char(encode_char_to_id(decode_id_to_char(encode_char_to_id(unique_characters)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461658fa",
   "metadata": {},
   "source": [
    "### Tokenize each character of the Tiny Shakespeare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a635ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Get data as a PyTorch tensor - each character is tokenized\n",
    "text_as_a_tensor = torch.tensor(encode_char_to_id(training_data), dtype=torch.long)\n",
    "\n",
    "# Make the tensor a numpy array, then decode each individual number as their respective character.\n",
    "#print(decode_id_to_char(text_as_a_tensor.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd05799",
   "metadata": {},
   "source": [
    "### Create train, validation, and test splits. Set up your code environment to integrate a data loader into the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aedf63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_len = len(text_as_a_tensor) \n",
    "train_split = text_as_a_tensor[:int(text_len*0.8)]\n",
    "val_split = text_as_a_tensor[int(text_len*0.8):int(text_len*0.9)]\n",
    "test_split = text_as_a_tensor[int(text_len*0.9):]\n",
    "#print(len(train_split))\n",
    "#print(len(val_split))\n",
    "#print(len(test_split))\n",
    "\n",
    "# The max amount of token ids to take into consideration when predicting output.\n",
    "# If we were using sub word tokenization, we would be able to consider up to 16 \n",
    "# sub words. However, since we are doing character based tokenization, we can \n",
    "# only consider up to 16 characters when predicting next output. We have to \n",
    "# truncate whenever the transformer gets more than {context_length} ids to \n",
    "# consider when predicting next output, as that is the upper limit to the \n",
    "# context length we set.\n",
    "context_length = 16\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt-from-scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
