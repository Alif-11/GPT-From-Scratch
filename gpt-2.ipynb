{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9f6cd67",
   "metadata": {},
   "source": [
    "### Get the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3b30b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data from the Tiny Shakespeare dataset):\n",
      "\n",
      "\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "Second Citizen:\n",
      "Would you proceed especially against Caius Marcius?\n",
      "\n",
      "All:\n",
      "Against him first: he's a very dog to the commonalty.\n",
      "\n",
      "Second Citizen:\n",
      "Consider you what services he has done for his country?\n",
      "\n",
      "First Citizen:\n",
      "Very well; and could be content to give him good\n",
      "report fort, but that he pays himself with being proud.\n",
      "\n",
      "Second Citizen:\n",
      "Nay, but speak not maliciously.\n",
      "\n",
      "First Citizen:\n",
      "I say unto you, what he hath done famously, he did\n",
      "it to that end: though soft-conscienced men can be\n",
      "content to say it was for his country he did it to\n",
      "please his mother and to be partly proud; which he\n",
      "is, even till the altitude of his virtue.\n",
      "\n",
      "Second Citizen:\n",
      "What he cannot help in his nature, you account a\n",
      "vice in him. You must in no way say he is covetous.\n",
      "\n",
      "First Citizen:\n",
      "If I must not, I need not be barren of accusations;\n",
      "he hath faults, with surplus, to tire in repetition.\n",
      "What shouts are these? The other side o' the city\n",
      "is risen: why stay we prating here? to the Capitol!\n",
      "\n",
      "All:\n",
      "Come, come.\n",
      "\n",
      "First Citizen:\n",
      "Soft! who comes here?\n",
      "\n",
      "Second Citizen:\n",
      "Worthy Menenius Agrippa; one that hath always loved\n",
      "the people.\n",
      "\n",
      "First Citizen:\n",
      "He's one honest enough: would all the rest were so!\n",
      "\n",
      "MENENIUS:\n",
      "What work's, my countrymen, in hand? where go you\n",
      "With bats and clubs? The matter? speak, I pray you.\n",
      "\n",
      "First Citizen:\n",
      "Our business is not unknown to the senate; they have\n",
      "had inkling this fortnight what we intend to do,\n",
      "which now we'll show 'em in deeds. They say poor\n",
      "suitors have strong breaths: they shall know we\n",
      "have strong arms too.\n",
      "\n",
      "MENENIUS:\n",
      "Why, masters, my good friends, mine honest neighbours,\n",
      "Will you undo yourselves?\n",
      "\n",
      "First Citizen:\n",
      "We cannot, sir, we are undone already.\n",
      "\n",
      "MENENIUS:\n",
      "I tell you, friends, most charitable care\n",
      "Have the patricians of you. For your wants,\n",
      "Your suffering in this dearth, you may as well\n",
      "Strike at the heaven with your staves as lift them\n",
      "Against the Roman state, whose course will on\n",
      "The way it takes, cracking ten thousand curbs\n",
      "Of more strong link asunder than can ever\n",
      "Appear in your impediment. For the dearth,\n",
      "The gods, not the patricians, make it, and\n",
      "Your knees to them, not arms, must help. Alack,\n",
      "You are transported by calamity\n",
      "Thither where more attends you, and you slander\n",
      "The helms o' the state, who care for you like fathers,\n",
      "When you curse them as enemies.\n",
      "\n",
      "First Citizen:\n",
      "Care for us! True, indeed! They ne'er cared for us\n",
      "yet: suffer us to famish, and their store-houses\n",
      "crammed with grain; make edicts for usury, to\n",
      "support usurers; repeal daily any wholesome act\n",
      "established against the rich, and provide more\n",
      "piercing statutes daily, to chain up and restrain\n",
      "the poor. If the wars eat us not up, they will; and\n",
      "there's all the love they bear us.\n",
      "\n",
      "MENENIUS:\n",
      "Either you must\n",
      "Confess yourselves wondrous malicious,\n",
      "Or be accused of folly. I shall tell you\n",
      "A pretty tale: it may be you have heard it;\n",
      "But, since it serves my purpose, I will venture\n",
      "To stale 't a little more.\n",
      "\n",
      "First Citizen:\n",
      "Well, I'll hear it, sir: yet you must not think to\n",
      "fob off\n"
     ]
    }
   ],
   "source": [
    "# Download training data text\n",
    "import numpy as np\n",
    "from typing import List, Any\n",
    "with open(\"./training-data.txt\", 'r') as training_data_file:\n",
    "  training_data = training_data_file.read()\n",
    "\n",
    "print(f\"\"\"Training data from the Tiny Shakespeare dataset):\\n\\n\n",
    "{training_data[:4000]}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23524094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data length: 1115393\n",
      "65\n",
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
     ]
    }
   ],
   "source": [
    "print(f\"training data length: {len(training_data)}\")\n",
    "\n",
    "## Get all unique characters from the training_data\n",
    "# Please note - the reasoning for this is that Andrej Karpathy wants to make a \n",
    "# next character predictor. Hence, we need all the characters, not the words.\n",
    "\n",
    "# Don't actually need list here, as sorted returns a new sorted list given an iterable.\n",
    "# We add the list(...) function anyway for readability.\n",
    "unique_characters = sorted(list(set(training_data))) \n",
    "print(len(unique_characters))\n",
    "print(''.join(unique_characters))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46fd8583",
   "metadata": {},
   "source": [
    "### Create our Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af6f18e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, ' ': 1, '!': 2, '$': 3, '&': 4, \"'\": 5, ',': 6, '-': 7, '.': 8, '3': 9, ':': 10, ';': 11, '?': 12, 'A': 13, 'B': 14, 'C': 15, 'D': 16, 'E': 17, 'F': 18, 'G': 19, 'H': 20, 'I': 21, 'J': 22, 'K': 23, 'L': 24, 'M': 25, 'N': 26, 'O': 27, 'P': 28, 'Q': 29, 'R': 30, 'S': 31, 'T': 32, 'U': 33, 'V': 34, 'W': 35, 'X': 36, 'Y': 37, 'Z': 38, 'a': 39, 'b': 40, 'c': 41, 'd': 42, 'e': 43, 'f': 44, 'g': 45, 'h': 46, 'i': 47, 'j': 48, 'k': 49, 'l': 50, 'm': 51, 'n': 52, 'o': 53, 'p': 54, 'q': 55, 'r': 56, 's': 57, 't': 58, 'u': 59, 'v': 60, 'w': 61, 'x': 62, 'y': 63, 'z': 64}\n",
      "{0: '\\n', 1: ' ', 2: '!', 3: '$', 4: '&', 5: \"'\", 6: ',', 7: '-', 8: '.', 9: '3', 10: ':', 11: ';', 12: '?', 13: 'A', 14: 'B', 15: 'C', 16: 'D', 17: 'E', 18: 'F', 19: 'G', 20: 'H', 21: 'I', 22: 'J', 23: 'K', 24: 'L', 25: 'M', 26: 'N', 27: 'O', 28: 'P', 29: 'Q', 30: 'R', 31: 'S', 32: 'T', 33: 'U', 34: 'V', 35: 'W', 36: 'X', 37: 'Y', 38: 'Z', 39: 'a', 40: 'b', 41: 'c', 42: 'd', 43: 'e', 44: 'f', 45: 'g', 46: 'h', 47: 'i', 48: 'j', 49: 'k', 50: 'l', 51: 'm', 52: 'n', 53: 'o', 54: 'p', 55: 'q', 56: 'r', 57: 's', 58: 't', 59: 'u', 60: 'v', 61: 'w', 62: 'x', 63: 'y', 64: 'z'}\n",
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
     ]
    }
   ],
   "source": [
    "# Similar to the Sebastian Raschka video:\n",
    "# Create a mapping between tokens and token ids, and vice versa\n",
    "# - although, again, in Andrej Karpathy's case, our tokens are characters, not words\n",
    "\n",
    "# Simple tokenizer\n",
    "char_to_id = {char:integer_id for char, integer_id in zip(unique_characters, range(len(unique_characters)))}\n",
    "id_to_char = {integer_id:char for char, integer_id in char_to_id.items()}\n",
    "print(char_to_id)\n",
    "print(id_to_char)\n",
    "\n",
    "def encode_char_to_id(string : str) -> List[int]:\n",
    "  return [char_to_id[char] for char in string]\n",
    "\n",
    "def decode_id_to_char(list_of_chars : List[int]) -> str:\n",
    "  return ''.join([id_to_char[integer_id] for integer_id in list_of_chars])\n",
    "#print(unique_characters)\n",
    "\n",
    "# encode_char_to_id and decode_id_to_char are inverse functions of each other\n",
    "print(decode_id_to_char(encode_char_to_id(decode_id_to_char(encode_char_to_id(unique_characters)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461658fa",
   "metadata": {},
   "source": [
    "### Tokenize each character of the Tiny Shakespeare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a635ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Get data as a PyTorch tensor - each character is tokenized\n",
    "text_as_a_tensor = torch.tensor(encode_char_to_id(training_data), dtype=torch.long)\n",
    "\n",
    "# Make the tensor a numpy array, then decode each individual number as their respective character.\n",
    "#print(decode_id_to_char(text_as_a_tensor.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd05799",
   "metadata": {},
   "source": [
    "### Create train, validation, and test splits. Set up your code environment to integrate a data loader into the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8aedf63b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([892314])\n",
      "torch.Size([111539])\n",
      "torch.Size([111540])\n"
     ]
    }
   ],
   "source": [
    "text_len = len(text_as_a_tensor) \n",
    "train_split = text_as_a_tensor[:int(text_len*0.8)]\n",
    "val_split = text_as_a_tensor[int(text_len*0.8):int(text_len*0.9)]\n",
    "test_split = text_as_a_tensor[int(text_len*0.9):]\n",
    "#print(len(train_split))\n",
    "#print(len(val_split))\n",
    "#print(len(test_split))\n",
    "\n",
    "# The max amount of token ids to take into consideration when predicting output.\n",
    "# If we were using sub word tokenization, we would be able to consider up to 16 \n",
    "# sub words. However, since we are doing character based tokenization, we can \n",
    "# only consider up to 16 characters when predicting next output. We have to \n",
    "# truncate whenever the transformer gets more than {context_length} ids to \n",
    "# consider when predicting next output, as that is the upper limit to the \n",
    "# context length we set.\n",
    "context_length = 16\n",
    "batch_size = 8\n",
    "\n",
    "print(train_split.shape)\n",
    "print(val_split.shape)\n",
    "print(test_split.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47b1662",
   "metadata": {},
   "source": [
    "### Create dataloader, which randomly obtains a batch of blocks of contiguous characters. The blocks must be context_length in size, and we'll have batch_size number of blocks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "3713bb9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "handsome stripli\n",
      "\n",
      "\n",
      "SICINIUS:\n",
      "This\n",
      "\n",
      "'Tis torture, a\n",
      "are men's ends m\n",
      "MILLO:\n",
      "Be advise\n",
      "not now have the\n",
      "t thought of wha\n",
      "ir: I have\n",
      "a kin\n"
     ]
    }
   ],
   "source": [
    "def get_batch(dataset, context_length, batch_size):\n",
    "  # We assume dataset to be a 1D tensor as of now.\n",
    "\n",
    "  # len(dataset) - context length means we dont start sampling a block that will\n",
    "  # be shorter than our context length\n",
    "  indices_to_begin_a_batch_from = torch.randint(len(dataset) - context_length, (batch_size,))\n",
    "  inputs = torch.stack([dataset[index:index+context_length] for index in indices_to_begin_a_batch_from], dim=0)\n",
    "  labels = torch.stack([dataset[index+1:index+context_length+1]\n",
    "                         for index in indices_to_begin_a_batch_from], dim=0)\n",
    "  return inputs, labels\n",
    "\n",
    "inputs, labels = get_batch(train_split, context_length, batch_size)\n",
    "#print(inputs)\n",
    "#print(labels)\n",
    "\n",
    "def visualize_input_label_intuition(inputs, labels, batch_size, context_length):\n",
    "  for batch_idx in range(batch_size):\n",
    "    for timestep in range(context_length):\n",
    "      x = inputs[batch_idx, :timestep+1]\n",
    "      y = labels[batch_idx, timestep]\n",
    "      #print(batch_idx)\n",
    "      #print(timestep)\n",
    "      #print(f\"When the inputs are {x.numpy()}, the label is {y.item()}\")\n",
    "\n",
    "#visualize_input_label_intuition(inputs, labels, batch_size, context_length)\n",
    "\n",
    "for tensor in inputs:\n",
    "  print(decode_id_to_char(tensor.numpy()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebd8f7d",
   "metadata": {},
   "source": [
    "### Implement the simplest language model to use with our data: the bigram language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53e992c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "  def __init__(self, vocab_size):\n",
    "    # vocab_size is the total number of tokens (in this case, our tokens are \n",
    "    # characters), that the tokenizer could possibly see.\n",
    "    super.__init__()\n",
    "\n",
    "    # In our bigram model, we want to look at the most likely next possible token\n",
    "    # that could result from our current token. In a trigram model, we may want to\n",
    "    # link from vocab_size * vocab_size input to vocab_size output. For n-gram, where\n",
    "    # n >= 2, we probably want mapping from (vocab_size) ^ [n - 1] to vocab_size. For token\n",
    "    # k_(n-2) k_(n-3) k_(n-4)..., we map from the tokens to input by doing k_i as token id + \n",
    "    # vocab_size ^ (i+1) (is the subscript value of k, where k is a token. The greatest token\n",
    "    # subscript is n-2, and it decreases as we go from left to right).\n",
    "    self.next_token_embedding_table_ = nn.Embedding(vocab_size, vocab_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt-from-scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
